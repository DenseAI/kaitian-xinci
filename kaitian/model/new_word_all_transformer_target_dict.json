"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 3, \"filters\": \"\\t\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"[UNK]\", \"document_count\": 0, \"word_counts\": \"{}\", \"word_docs\": \"{}\", \"index_docs\": \"{}\", \"index_word\": \"{\\\"1\\\": \\\"[CLS]\\\", \\\"2\\\": \\\"[UNK]\\\"}\", \"word_index\": \"{\\\"[CLS]\\\": 1, \\\"[UNK]\\\": 2}\"}}"